{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n10 Academy Cohort A\\nWeekly Challenge: Week 6\\nPrecision RAG: Prompt Tuning For Building Enterprise Grade RAG Systems\\n\\nBusiness objective  \\nPromptlyTech is an innovative e-business specializing in providing AI-driven solutions for optimizing the use of Language Models (LLMs) in various industries. The company aims to revolutionize how businesses interact with LLMs, making the technology more accessible, efficient, and effective. By addressing the challenges of prompt engineering, the company plays a pivotal role in enhancing decision-making, operational efficiency, and customer experience across various industries. PromptlyTech\\'s solutions are designed to cater to the evolving needs of a digitally-driven business landscape, where speed and accuracy are key to staying competitive.\\nThe company focuses on key services: Automatic Prompt Generation, Automatic Test Case Generation, and Prompt Testing and Ranking.\\n1. Automatic Prompt Generation Service:\\n    • This service streamlines the process of creating effective prompts, enabling businesses to efficiently utilize LLMs for generating high-quality, relevant content. It significantly reduces the time and expertise required in crafting prompts manually.\\n2. Automatic Test Case Generation Service:\\n    • PromptlyTech’s service automates the generation of diverse test cases, ensuring comprehensive coverage and identifying potential issues. This enhances the reliability and performance of LLM applications, saving significant time in the QA(Quality Assurance) process.\\n3. Prompt Testing and Ranking Service:\\n    • PromptlyTech’s service evaluates and ranks different prompts based on effectiveness, helping Users to get the desired outcome from LLM. It ensures that chatbots and virtual assistants provide accurate, contextually relevant responses, thereby improving user engagement and satisfaction.\\nBackground Context\\n\\nIn the evolving field of artificial intelligence, Language Models (LLMs) like GPT-3.5 and GPT-4 have become crucial for various applications. Their effectiveness, however, heavily depends on the quality of the prompts they receive, leading to the emergence of \"prompt engineering\" as a key skill.\\n\\nPrompt engineering is the craft of designing queries or statements to guide LLMs to produce desired outcomes. The challenge lies in the sensitivity of these models to prompt nuances, where slight variations can yield vastly different results. This poses a significant hurdle for users, especially in business contexts where accuracy and relevance are paramount.\\n\\nThe need for simplified, efficient prompt engineering is clear. Automating and optimizing this process can save time, enhance LLM productivity, and make advanced AI capabilities more accessible to a broader range of users. The tasks of Automatic Prompt Generation, Test Case Generation, and Prompt Testing and Ranking are aimed at addressing these challenges, streamlining the prompt engineering process for more effective use of LLMs.\\nLearning Outcomes\\nSkills Development\\n    • Prompt Engineering Proficiency: Gain expertise in crafting effective prompts that guide LLMs to desired outputs, understanding nuances and variations in language that impact model responses.\\n    • Critical Analysis: Develop the ability to critically analyze and evaluate the effectiveness of different prompts based on their performance in varied scenarios.\\n    • Technical Aptitude with LLMs: Enhance technical skills in using advanced language models like GPT-4 and GPT-3.5-Turbo, understanding their functionalities and capabilities.\\n    • Problem-Solving and Creativity: Cultivate creative problem-solving skills by generating innovative prompts and test cases, addressing complex and varied objectives.\\n    • Data Interpretation: Learn to interpret and analyze data from test cases and prompt evaluations, deriving meaningful insights from performance metrics.\\n\\nKnowledge Acquisition\\n    • Understanding of Language Models: Acquire a deeper understanding of how LLMs function, including their strengths, limitations, and the principles behind their responses.\\n    • Insights into Automated Test Case Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.\\n    • ELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.\\n    • Prompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.\\n    • Industry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges.\\n\\nTeam\\nTutors: \\n    • Yabebal\\n    • Emitinan\\n    • Rehmet\\nBadges\\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\\n\\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\\n\\nVisualization - quality of visualizations, understandability, skimmability, choice of visualization\\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD/CML\\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\\nMost supportive in the community - helping others, adding links, tutoring those struggling\\n\\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\\n\\nGroup Work Policy\\nEveryone has to submit all their work individually. \\nInstruction: Automatic Prompt Engineering\\nFundamental Tasks\\nThe core tasks for this week’s challenge in Automatic Prompt Engineering are outlined below:\\n\\n    1. Understand Prompt Engineering Tools and Concepts: Gain a thorough understanding of the tools and theoretical concepts involved in prompt engineering for Language Models (LLMs).\\n\\n    2. Familiarize with Language Models: Learn about the capabilities and functionalities of advanced LLMs like GPT-4 and GPT-3.5-Turbo.\\n\\n    3. Develop a Plan for Prompt Generation and Testing: Create a comprehensive plan that outlines the approach for automated prompt generation, test case creation, and prompt evaluation.\\n\\n    4. Set Up a Development Environment: Prepare a suitable development environment that supports the integration and testing of LLMs in the prompt engineering process.\\n\\n    5. Design User Interface for Prompt System: Plan and initiate the development of a user-friendly interface for prompt input, refinement, and performance analysis.\\n\\n    6. Plan Integration of LLMs: Strategize the integration of LLMs into the prompt system for automated generation and testing.\\n\\n    7. Build and Refine Prompt Generation System: Develop the automated prompt generation system, ensuring it aligns with user inputs and objectives.\\n\\n    8. Develop Automatic Test Case Generation System: Create a system for generating test cases that evaluate the effectiveness of prompts in various scenarios.\\n\\n    9. Implement Prompt Testing and Evaluation Mechanism: Set up testing procedures using Monte Carlo matchmaking and ELO rating systems to evaluate and rank prompts.\\n\\n    10. Refine and Optimize System Based on Feedback: Continuously refine the prompt generation and evaluation system based on user feedback and performance data.\\n\\n\\n\\n\\n\\n\\n\\nTask 1: Review the Evolution of Automatic Prompt Engineering\\nFocus on understanding the key developments in the field of automatic prompt engineering for Language Models (LLMs).\\n\\nStudy Key Concepts and Tools:\\n    • Understand the key components of an enterprise grade RAG systems\\n        ◦ Retrieval-augmented generation (RAG): What it is and why it’s a hot topic for enterprise AI\\n        ◦ Advanced RAG for LLMs/SLMs\\n        ◦ RAG for Text Generation Processes in Businesses (check part 1, 3, & 4 as well) \\n        ◦ Langchain Reterivers\\n    • Understand the need for advanced prompt engineering in building enterprise grade RAG systems\\n        ◦ Full Fine-Tuning, PEFT, Prompt Engineering, and RAG: Which One Is Right for You?\\n        ◦ Advanced Prompt Engineering - Practical Examples\\n        ◦ Prompt Engineering 201: Advanced methods and toolkits\\n        ◦ Do you agree with this article? RAG is Just Fancier Prompt Engineering\\n    • Understand the need for evaluating RAG components\\n        ◦ An Overview on RAG Evaluation\\n        ◦ Evaluating RAG: Using LLMs to Automate Benchmarking of Retrieval Augmented Generation Systems\\n        ◦ Evaluating RAG Applications with RAGAs\\n        ◦ RAG Evaluation Using LangChain and Ragas\\n        ◦ RAG System: Metrics and Evaluation Analysis with LlamaIndex\\n        ◦ Evaluating RAG Part I: How to Evaluate Document Retrieval\\n        ◦ Evaluating RAG/LLMs in highly technical settings using synthetic QA generation\\n        ◦ Evaluating Multi-Modal RAG\\n    • Understand the tools and techniques to automatically generate RAG evaluation data \\n        ◦ The Tech Buffet #16: Quickly Evaluate your RAG Without Manually Labeling Test Data\\n        ◦ Generating a Synthetic Dataset for RAG\\n        ◦ \\n    • Learn key packages to planning, building, testing, monitoring, and deploying enterprise grade RAG system\\n        ◦ Iterate on LLMs faster: Measure LLM quality and catch regressions\\n        ◦ Building RAG-based LLM Applications for Production\\n        ◦ ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\\n    • Understand the end-to-end technology stack of RAG systems\\n        ◦ End-to-End LLMOps Platform\\n        ◦ An Enterprise-Grade Reference Architecture for the Production Deployment of LLMs Using the RAG Pattern on Azure OpenAI\\nTask 2: Design and Develop the Prompt Generation System\\n    • Users can input a description of their objective or task and specify a few scenarios along with their expected outputs. \\n    • Write or adopt sophisticated algorithms, you generate multiple prompt options based on the provided information. \\n    • This automated prompt generation process saves time and provides a diverse range of alternatives to consider. But add an evaluation metrics that check whether the generated prompt candidate aligns with the input description.\\nTask 3: Implement Test Case Generation and Evaluation\\nTo further enhance the prompt generation process, incorporate automatic test case generation. \\n    • By analysing the description provided by the user,  create a set of test cases that serve as evaluation benchmarks for the prompt candidates.\\n    • These test cases simulate various scenarios, enabling users to observe how each prompt performs in different contexts. \\n    • The generated test cases serve as a starting point, sparking creativity and inspiring additional test cases for comprehensive evaluation.\\nTask 4 : Prompt Testing and Ranking\\nGoals\\nComprehensive Evaluation: Provide a robust system that uses various methodologies for a thorough assessment of prompts.\\nCustomizable and User-Centric: Allow users to choose or customize their preferred evaluation methods.\\nDynamic and Adaptive: Ensure the system remains flexible and adaptive, capable of incorporating new ranking methodologies as they emerge.\\n\\nPrimary Methods\\n\\n    • Monte Carlo Matchmaking: This method is used to select and match different prompt candidates against each other. The Monte Carlo method, known for its applications in problem-solving and decision-making processes, helps in optimizing the information gained from each prompt battle. By simulating various matchups, it allows the system to test the effectiveness of each prompt in different scenarios.\\n    • ELO Rating System:  This system, which is commonly used in chess and other competitive games, rates the prompts based on their performance in the battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the strength of the opponents each prompt has defeated. This rating helps in objectively ranking the prompts based on their effectiveness.\\n\\nAdditional Ranking and Matching Mechanisms\\n    • TrueSkill Rating System: Ideal for scenarios involving multiple competitors, adjusting ratings based on not just wins and losses but also the uncertainty in performance.\\n    • Glicko Rating System: Similar to ELO but with added flexibility, accounting for the volatility in a player\\'s (or prompt’s) performance and the reliability of their rating.\\n    • Bayesian Rating Systems: Applies Bayesian inference for a probabilistic approach to rating, considering uncertainties and contextual variations in prompt performance.\\n    • Pairwise Comparison Methods: Involves direct comparisons between pairs of prompts, potentially integrating user preferences or expert evaluations into the ranking process.\\n    • Categorical Ranking: Instead of a numerical rating, prompts are categorized based on performance criteria like creativity, relevance, etc., for more qualitative assessments.\\n    • Adaptive Ranking Algorithms: Algorithms that learn and adjust over time, considering historical performance data and evolving user preferences or requirements.\\n    • Semantic Similarity Matching: Using NLP techniques to match prompts based on semantic content, ideal for understanding nuanced differences in prompt effectiveness.\\n\\nYou should adopt an innovative approach to prompt evaluation by utilizing Monte Carlo matchmaking and  ELO rating systems, or any alternative method to match and rank.\\nTask 5: User Interface Development\\nDevelop a user-friendly interface for interacting with the prompt engineering system.\\n    • UI Design: Plan and design a user interface that allows users to easily input data, receive prompts, and view evaluation results.\\n    • UI Implementation: Develop and integrate the user interface with the backend prompt engineering system.\\nTask 6: System Integration and Testing\\n    • Integrate all components of the system and conduct comprehensive testing.\\n    • Integrate the prompt generation, test case generation, evaluation, and user interface components.\\n    • Test the entire system for functionality, usability, and performance. Refine based on feedback and test results.\\n\\n\\nTutorials Schedule\\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\\nMonday: Understanding Prompt engineering \\nHere the trainees will understand the week’s challenge.\\n    • Introduction to Week Challenge (Yabebal)\\n    • Introduction and challenge to prompt engineering (Fikerte)\\n\\nKey Performance Indicators:\\n\\n    • Understanding week’s challenge\\n    • Understanding the prompt engineering\\n    • Ability to reuse previous knowledge\\nTuesday\\n    • RAG components (Rehmet)\\n    • Techniques to improving R (Retrievers) in RAG (Emitnan)\\n\\nKey Performance Indicators:\\n\\n    • Understanding Prompt ranking \\n    • Understanding prompt matching \\n    • Ability to reuse previous knowledge\\n\\nWednesday\\n    • RAG evaluation data generation (Abel)\\n    • Understanding of prompt matching and ranking (Mahlet)\\nThursday\\n    • RAG evaluation metrics (Emitnan)\\n    • RAGObs - DevObs of RAG development and production deployment  \\n\\nDeliverables\\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT SUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of your report not a link. \\nInterim Submission - Wednesday 8pm UTC\\n    • Link to your code in GitHub\\n        ◦ Repository where you will be using to complete the tasks in this week\\'s challenge. A minimum requirement is that you have a well structured repository and some coding progress is made.\\n\\n\\n    • A review report of your reading and understanding of Task 1 and any progress you made in other tasks. \\nFeedback\\nYou may not receive detailed comments on your interim submission, but will receive a grade.\\nFinal Submission - Saturday 8pm UTC\\n    • Link to your code in GitHub \\n        ◦ Complete work  for Automatic prompt generation\\n        ◦ Complete work  for Automatic evaluation \\n        ◦ Complete work for test case generation\\n\\n    • A blog post entry (which you can submit for example to Medium publishing) or a pdf report. . \\nFeedback\\nYou will receive comments/feedback in addition to a grade.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReferences\\n    • Meistrari didn’t see a good solution for prompt engineering, so it’s building one\\n    • AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\n    • Large Language Models Are Human-Level Prompt Engineers\\n    • Prompt Engineering\\n    • How to Create a Monte Carlo Simulation using Python\\n    • Monte Carlo Method Explained\\n    • What is Monte Carlo Simulation? How does it work?\\n    • Elo Rating Algorithm\\n    • Elo algorithm implementation in Python\\n    • TrueSkillTM: A Bayesian skill rating system\\n\\n\\n\\n', metadata={'source': 'file.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"file.txt\")\n",
    "loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1763, which is longer than the specified 1000\n",
      "Created a chunk of size 1344, which is longer than the specified 1000\n",
      "Created a chunk of size 3634, which is longer than the specified 1000\n",
      "Created a chunk of size 1263, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=80)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='10 Academy Cohort A\\nWeekly Challenge: Week 6\\nPrecision RAG: Prompt Tuning For Building Enterprise Grade RAG Systems', metadata={'source': 'file.txt'}),\n",
       " Document(page_content=\"Deliverables\\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT SUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of your report not a link. \\nInterim Submission - Wednesday 8pm UTC\\n    • Link to your code in GitHub\\n        ◦ Repository where you will be using to complete the tasks in this week's challenge. A minimum requirement is that you have a well structured repository and some coding progress is made.\\n\\n\\n    • A review report of your reading and understanding of Task 1 and any progress you made in other tasks. \\nFeedback\\nYou may not receive detailed comments on your interim submission, but will receive a grade.\\nFinal Submission - Saturday 8pm UTC\\n    • Link to your code in GitHub \\n        ◦ Complete work  for Automatic prompt generation\\n        ◦ Complete work  for Automatic evaluation \\n        ◦ Complete work for test case generation\", metadata={'source': 'file.txt'}),\n",
       " Document(page_content='In the evolving field of artificial intelligence, Language Models (LLMs) like GPT-3.5 and GPT-4 have become crucial for various applications. Their effectiveness, however, heavily depends on the quality of the prompts they receive, leading to the emergence of \"prompt engineering\" as a key skill.\\n\\nPrompt engineering is the craft of designing queries or statements to guide LLMs to produce desired outcomes. The challenge lies in the sensitivity of these models to prompt nuances, where slight variations can yield vastly different results. This poses a significant hurdle for users, especially in business contexts where accuracy and relevance are paramount.', metadata={'source': 'file.txt'}),\n",
       " Document(page_content='Team\\nTutors: \\n    • Yabebal\\n    • Emitinan\\n    • Rehmet\\nBadges\\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\\n\\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\\n\\nVisualization - quality of visualizations, understandability, skimmability, choice of visualization\\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD/CML\\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\\nMost supportive in the community - helping others, adding links, tutoring those struggling\\n\\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.', metadata={'source': 'file.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.get_relevant_documents(\"Saturday 8pm \")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def data_retriever(file, query):\n",
    "    # Load the document, split it into chunks, embed each chunk, and load it into the vector store.\n",
    "    raw_documents = TextLoader(file).load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    \n",
    "    # Assuming OpenAIEmbeddings is properly initialized and available as `embeddings`\n",
    "    db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    \n",
    "    query = query\n",
    "    docs = db.similarity_search(query)\n",
    "    \n",
    "    # Print each output individually\n",
    "    for i in range(min(4, len(docs))):  # Avoid index out of range if there are less than 4 results\n",
    "        print(docs[i].page_content)\n",
    "    \n",
    "    # Return None or any other value as needed\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1203, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\n",
      "\n",
      "When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\n",
      "Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\n",
      "\n",
      "When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\n",
      "Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\n",
      "\n",
      "When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\n",
      "I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously aware of much we're seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that's a water droplet\" without telling you details like where the lightest and darkest points are, or \"that's a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\n",
      "\n",
      "This is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\n"
     ]
    }
   ],
   "source": [
    "context = data_retriever('file.txt','brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_docs\u001b[39m(docs):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[1;32m     20\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mretriever\u001b[49m \u001b[38;5;241m|\u001b[39m format_docs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m|\u001b[39m model\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did the president say about technology?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: brown\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What did the president say about technology?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
